{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ceb60ba",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This example demostrates an end-to-end stream processing of live documents.\n",
    "- Part 1 configures AWS credentials, S3 paths, etc.\n",
    "- Part 2 prepares DynamoDB tables for streaming jobs and Search Engine queries.\n",
    "- Part 3 creats streaming jobs to process live documents.\n",
    "- Part 4 simulates a live stream of document by copying files to a live folder.\n",
    "- Part 5 implements a Flask App that acts like a Search Engine. \n",
    "\n",
    " \n",
    "# Part 1. Configurations\n",
    "In this module, we configure out credentials for AWS access, install necessary libaraies, and import functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7835eb64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30f94f639c764172ac3b5ffc58264930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "########### Configurations #######\n",
    "### Your credentials (copy from Learner Lab)\n",
    "aws_access_key_id=\"ASIA2DIJSV6FV7VPAAJ2\"\n",
    "aws_secret_access_key=\"NnfbbTpTCXGpMQNSrSkJeASu6BCMB5XE/tbCLifR\"\n",
    "aws_session_token=\"IQoJb3JpZ2luX2VjEC4aCXVzLXdlc3QtMiJIMEYCIQDzOS7ItviUGvGRi6AuSQEHfixiUYeeCfEiz4XF4DMZ5QIhAOSH62oCiDXg2+lAHJBb3fR6DaFTAbmS8IklA+2Nh6toKroCCMf//////////wEQARoMNjk0MTk0MTg4MTcxIgySDoRos5y3sf0JQ2gqjgIFswgKAtpMo2fDA1IsqMVuTn4OwGp1/6Qo4LL5811ITcILdV++gU5XIrBYPicPAXWyDoIoQGO3wCZ4Yhz0zqbVK4ekQ7v9QwMfW8DEZTgruympC+mVh7hjEKr73doEEfgICzc3mxf2qN2mRUjPP2gQdvLw3aSUrfjzim0tTqQweh0ffUsfd8DpnfnRPNzImCCMb2ovfNEfrL6KyzI9F4EZXZq+q7Pt/O01/61KjNNKKZ4DdAzOUZmnHtYHFeDkSlJRgA4No6nuJ4b061o/lB+qmaAOULWuut64eNGo50cwZ//L03V2YGXqlnilj2l83XxpDRVy/IQafSZbGn2D0POvGXTeoj6K6kZJAySaSPYwu4+4swY6nAFD7sleCDgMddx/S5x18aIcPRqo5ibekriaERNyGv9ASx2g6Xvopyp10suTmF7cy3gPMswU47NSoz700hWMd0I5H/uYfnguxdS6r4N1JTX13XLogWpwjp1aqHTxnbS21ouIby9U7ByrGe9gwFTGSf/8iArNXzgfZvlX8yAa6iHMQ5xj2XYOOL2MPHzD16tmcvjYau/Mz7fUuhpnaPA=\"\n",
    "### Your bucket, textcorpora folder, and streaming folder\n",
    "bucket_name = 'aws-emr-studio-694194188171-us-east-1'\n",
    "# textcorpora_folder = 'week10/textcorpora/'     # books with complete content\n",
    "textcorpora_folder = 'aws-emr-studio-694194188171-us-east-1/1718490117661/textcorpora-small/' # books with few lines only (speedup testing)\n",
    "streaming_folder = 'aws-emr-studio-694194188171-us-east-1/live_docs/'           # the simulated live input source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7408fc27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bea1532acb0847be97018bf57a1c61b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Package already installed for current Spark context!\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1718490297271_0002/container_1718490297271_0002_01_000001/pyspark.zip/pyspark/context.py\", line 2602, in install_pypi_package\n",
      "    raise ValueError(\"Package already installed for current Spark context!\")\n",
      "ValueError: Package already installed for current Spark context!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# install necessary library boto3 so that we can access aws resources\n",
    "sc.install_pypi_package('boto3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d153dff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b0b799a5a06442185be1f487e4cea59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import threading\n",
    "import time\n",
    "import random\n",
    "import boto3\n",
    "\n",
    "from pyspark.sql.types import StructField, StructType, StringType\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5605cc95",
   "metadata": {},
   "source": [
    "# Part 2. DynamoDB\n",
    "This part prepares and sets up two tables that later serves as the streaming sink.\n",
    "Our streaming jobs will write output to these tables and later can be used for TFIDF calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20585d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8227ec8b15f24c19b8b4c7b75a02bce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table tbl_token_count already exists\n",
      "Table tbl_docsize already exists"
     ]
    }
   ],
   "source": [
    "def get_dynamodb():\n",
    "    '''\n",
    "    Create an instance for interacting with Amazon DynamoDB.\n",
    "    '''\n",
    "    return boto3.resource('dynamodb',\n",
    "                 aws_access_key_id=aws_access_key_id,\n",
    "                 aws_secret_access_key=aws_secret_access_key,\n",
    "                 aws_session_token=aws_session_token,\n",
    "                 region_name=\"us-east-1\")\n",
    "\n",
    "def create_token_count_table(dynamodb, table_name='tbl_token_count', primary_key='docid', sort_key='token'):\n",
    "    '''\n",
    "    Create token_count table where primary_key='docid', sort_key='token', and value is the token count in that doc\n",
    "    '''\n",
    "    \n",
    "    if dynamodb is None:\n",
    "        dynamodb = get_dynamodb()\n",
    "\n",
    "    existing_tables = dynamodb.meta.client.list_tables()['TableNames']\n",
    "    if table_name not in existing_tables:\n",
    "        print(f\"Creating table {table_name}\")\n",
    "        # Define the attribute definitions for the primary key and sort key\n",
    "        attribute_definitions = [ {'AttributeName': primary_key, 'AttributeType': 'S'},\n",
    "                                  {'AttributeName': sort_key,    'AttributeType': 'S'} ]\n",
    "\n",
    "        # Define the key schema for the table\n",
    "        key_schema = [ {'AttributeName': primary_key, 'KeyType': 'HASH'},\n",
    "                       {'AttributeName': sort_key,    'KeyType': 'RANGE'} ]\n",
    "\n",
    "        # Define the provisioned throughput capacity for the table\n",
    "        provisioned_throughput = {'ReadCapacityUnits': 5, 'WriteCapacityUnits': 5}\n",
    "\n",
    "        # Create the table\n",
    "        response = dynamodb.create_table(\n",
    "                TableName=table_name,\n",
    "                AttributeDefinitions=attribute_definitions,\n",
    "                KeySchema=key_schema,\n",
    "                ProvisionedThroughput=provisioned_throughput\n",
    "        )\n",
    "\n",
    "        print(\"Waiting for table to be ready\")\n",
    "    else:\n",
    "        print(f'Table {table_name} already exists')\n",
    "        \n",
    "def create_docsize_table(dynamodb, table_name='tbl_docsize', primary_key='docid'):\n",
    "    '''\n",
    "    Create docsize table where key is docid and value is total token in the doc\n",
    "    '''\n",
    "    \n",
    "    if dynamodb is None:\n",
    "        dynamodb = get_dynamodb()\n",
    "\n",
    "    existing_tables = dynamodb.meta.client.list_tables()['TableNames']\n",
    "    if table_name not in existing_tables:\n",
    "        print(f\"Creating table {table_name}\")\n",
    "        # Define the attribute definitions for the primary key and sort key\n",
    "        attribute_definitions = [ {'AttributeName': primary_key, 'AttributeType': 'S'} ]\n",
    "\n",
    "        # Define the key schema for the table\n",
    "        key_schema = [ {'AttributeName': primary_key, 'KeyType': 'HASH'} ]\n",
    "\n",
    "        # Define the provisioned throughput capacity for the table\n",
    "        provisioned_throughput = {'ReadCapacityUnits': 5, 'WriteCapacityUnits': 5}\n",
    "\n",
    "        # Create the table\n",
    "        response = dynamodb.create_table(\n",
    "                TableName=table_name,\n",
    "                AttributeDefinitions=attribute_definitions,\n",
    "                KeySchema=key_schema,\n",
    "                ProvisionedThroughput=provisioned_throughput\n",
    "        )\n",
    "\n",
    "        print(\"Waiting for table to be ready\")\n",
    "    else:\n",
    "        print(f'Table {table_name} already exists')\n",
    "    \n",
    "                \n",
    "def write_to_dynamodb_token_count(row):\n",
    "    dynamodb.Table('tbl_token_count').put_item(\n",
    "        Item={\n",
    "            \"docid\": str(row['docid']),\n",
    "            \"token\": str(row['token']),\n",
    "            \"tokencount\":  int(row['token_count'])\n",
    "        }\n",
    "    )\n",
    "    \n",
    "def write_to_dynamodb_token_count_batch(batch_df, batch_id):\n",
    "    rows = batch_df.collect()\n",
    "    for row in rows:\n",
    "        dynamodb.Table('tbl_token_count').put_item(\n",
    "            Item={\n",
    "                \"docid\": str(row['docid']),\n",
    "                \"token\": str(row['token']),\n",
    "                \"tokencount\":  int(row['token_count'])\n",
    "            }\n",
    "        )\n",
    "     \n",
    "        \n",
    "def write_to_dynamodb_docsize(row):\n",
    "    dynamodb = get_dynamodb()\n",
    "    \n",
    "    dynamodb.Table('tbl_docsize').put_item(\n",
    "        Item={\n",
    "            \"docid\": str(row['docid']),\n",
    "            \"size\":  int(row['doc_size'])\n",
    "        }\n",
    "    )\n",
    "        \n",
    "# create an instance of dynamodb      \n",
    "dynamodb = get_dynamodb()\n",
    "\n",
    "# create two tables as sinks\n",
    "create_token_count_table(dynamodb)\n",
    "create_docsize_table(dynamodb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9cf3afe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de9fc9b06cb64a0cbd6df2d94a0c8152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ScalableTargetARN': 'arn:aws:application-autoscaling:us-east-1:694194188171:scalable-target/0d26c1583ff229284a73b83e8a76a8f97be5', 'ResponseMetadata': {'RequestId': 'ea415818-f212-478e-bad3-7564b7f67851', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'ea415818-f212-478e-bad3-7564b7f67851', 'content-type': 'application/x-amz-json-1.1', 'content-length': '131', 'date': 'Sat, 15 Jun 2024 23:08:05 GMT'}, 'RetryAttempts': 0}}"
     ]
    }
   ],
   "source": [
    "# Auto-scaling in DynamoDB allows you to automatically adjust the read and write throughput capacity\n",
    "# of your DynamoDB tables to handle changes in traffic without manual intervention. \n",
    "autoscaling_client = boto3.client('application-autoscaling',\n",
    "                                 aws_access_key_id=aws_access_key_id,\n",
    "                                 aws_secret_access_key=aws_secret_access_key,\n",
    "                                 aws_session_token=aws_session_token,\n",
    "                                 region_name=\"us-east-1\")\n",
    "#Write capacity with auto scaling\n",
    "autoscaling_client.register_scalable_target(ServiceNamespace='dynamodb',\n",
    "                                            ResourceId='table/tbl_token_count',\n",
    "                                            ScalableDimension='dynamodb:table:WriteCapacityUnits',\n",
    "                                            MinCapacity=1,\n",
    "                                            MaxCapacity=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3620464",
   "metadata": {},
   "source": [
    "# Part 3. Spark Streaming\n",
    "This module creates two Spark streaming jobs which write to two different DynamoDB tables.\n",
    "- Step 1 to 3 process the streamed data for word counting, tfidf calculation, etc.\n",
    "- Step 4 saves `(docid, totalcount)` into `tbl_docsize`, where the key is the `docid` and `totalcount` is the total number of tokens in the doc. It is used later for TF calculation. \n",
    "- Step 5 saves `(docid, token, tokencount)` into `tbl_token_count`, where is primary key is `docid`, sort key is `token`, and `tokencount` is the number of times this token appeared in the doc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4dc32022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "474908c19a534af4b4251f964e761da0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create stream dataframe by reading files from our S3 streaming folder\n",
    "booksSchema  = StructType( [StructField(\"line\", StringType(), False)] )\n",
    "streaming_df = spark.readStream\\\n",
    "                    .format(\"text\")\\\n",
    "                    .schema(booksSchema)\\\n",
    "                    .option(\"maxFilesPerTrigger\", 1)\\\n",
    "                    .option(\"path\", f's3://aws-emr-studio-694194188171-us-east-1/1718490117661/live_docs/')\\\n",
    "                    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e9f9fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b62a21f5cfe94cbfab129a46957837c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# step 1: extract and create docid from streamed dataframe\n",
    "# +--------------------+-----------+\n",
    "# |                line|      docid|\n",
    "# +--------------------+-----------+\n",
    "# |[Emma by Jane Aus...|austen-emma|\n",
    "# |                    |austen-emma|\n",
    "# |            VOLUME I|austen-emma|\n",
    "d1 = streaming_df.withColumn(\"filename\", F.input_file_name())\\\n",
    "                 .withColumn('docid', F.regexp_extract(F.col(\"filename\"), r'.*\\/(.+)\\.txt', 1))\\\n",
    "                 .drop('filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38e69e4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "503e28df488740fbbf0f6f0c245585bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# step 2: split document column to words and explode to word\n",
    "# +-------------+-----------+\n",
    "# |        docid|       word|\n",
    "# +-------------+-----------+\n",
    "# |carroll-alice|   [Alice's|\n",
    "# |carroll-alice| Adventures|\n",
    "d2 = d1.withColumn(\"words\", F.split(\"line\", \"\\s+\"))\n",
    "d3 = d2.withColumn('word', F.explode(F.col('words'))).drop('document').drop('words').drop('line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76bb9206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f83e8e7295b7499a8ea843b427b3f5c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# step 3: tokenize: lowercase -> letter only -> remove empty token\n",
    "# +-----------+------------+\n",
    "# |      docid|       token|\n",
    "# +-----------+------------+\n",
    "# |blake-poems|       poems|\n",
    "# |blake-poems|          by|\n",
    "# |blake-poems|     william|\n",
    "d4 = d3.withColumn('lower_word', F.lower(d3['word'])).drop('word')\n",
    "d5 = d4.withColumn('token', F.regexp_replace(d4[\"lower_word\"], \"[^a-z]\", \"\")).drop('lower_word')\n",
    "d6 = d5.where(F.col('token') != '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24b6944f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e0b37cb4e8442298db2054b3bfa6c9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# step 4: get total token in a doc and write to tbl_doc_size on DynamoDB\n",
    "# +-------------------+--------+\n",
    "# |              docid|doc_size|\n",
    "# +-------------------+--------+\n",
    "# |        blake-poems|    6817|\n",
    "# |burgess-busterbrown|   15864|\n",
    "doc_size = d6.groupBy('docid').count().withColumnRenamed(\"count\", \"doc_size\")\n",
    "\n",
    "# WRITE JOB 1: this will write the above df into the table tbl_docsize\n",
    "doc_size_query = doc_size.toDF(\"docid\", \"doc_size\")\\\n",
    "    .writeStream\\\n",
    "    .foreach(write_to_dynamodb_docsize) \\\n",
    "    .outputMode(\"update\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de51d34c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0d9b67fb31841a7929863cc210746be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# step 5: get token count per doc and write to tbl_token_count on DynamoDB \n",
    "# +-------------------+----------+-----------+\n",
    "# |              docid|     token|token_count|\n",
    "# +-------------------+----------+-----------+\n",
    "# |        blake-poems|      lick|          2|\n",
    "# |burgess-busterbrown|  sunbeams|          4|\n",
    "docid_token_count = d6.groupBy(['docid', 'token']).count().withColumnRenamed(\"count\", \"token_count\")\n",
    "\n",
    "# WRITE JOB 2: this will write the above df into the table tbl_token_count\n",
    "docid_token_count_query = docid_token_count.toDF(\"docid\", \"token\", \"token_count\")\\\n",
    "    .writeStream\\\n",
    "    .foreachBatch(write_to_dynamodb_token_count_batch) \\\n",
    "    .outputMode(\"update\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3606125",
   "metadata": {},
   "source": [
    "# Part 4. Live Data Simulation\n",
    "This module simulates a stream of data by copying a document to a live folder.\n",
    "In practice, the live folder could be where the logs/transcations/clicks are stored.\n",
    "\n",
    "The cell below starts a new thread that copies one file per minute until all 18 files are copied to the live folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1225461d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16bcb5e0ee024a9aa3d6fe6716f0bf4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "################################## Data Streaming Simulation ###############################\n",
    "def copy_document_to_live_folder():\n",
    "    '''\n",
    "    Simluate the date stream: randomly select a document from textcorpora folder\n",
    "                              and copy it to streaming folder\n",
    "    '''\n",
    "    s3 = boto3.client('s3')\n",
    "\n",
    "    # List the files in the source folder\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=textcorpora_folder)\n",
    "    files = response['Contents']\n",
    "\n",
    "    # Randomly select a file\n",
    "    selected_file = random.choice(files)\n",
    "\n",
    "    # Construct the source and destination object keys\n",
    "    source_key = selected_file['Key']\n",
    "    destination_key = streaming_folder + source_key.split('/')[-1]\n",
    "    \n",
    "    # Check if the file already exists in the destination folder\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=destination_key)\n",
    "    file_exists = 'Contents' in response\n",
    "\n",
    "    # Copy the file to the destination folder if it doesn't exist\n",
    "    if not file_exists:\n",
    "        s3.copy_object(\n",
    "            Bucket=bucket_name,\n",
    "            Key=destination_key,\n",
    "            CopySource={'Bucket': bucket_name, 'Key': source_key}\n",
    "        )\n",
    "        print(f'{source_key} is copied to streaming folder.')\n",
    "        \n",
    "\n",
    "def data_stream_simulator():\n",
    "    '''\n",
    "    Copy a file to streaming folder every 60 seconds to simulate the data streaming\n",
    "    '''\n",
    "    global stop_requested\n",
    "    while not stop_requested:\n",
    "        copy_document_to_live_folder()\n",
    "        time.sleep(60)\n",
    "        \n",
    "stop_requested = False\n",
    "livedata_thread = threading.Thread(target=data_stream_simulator)\n",
    "livedata_thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7dd182",
   "metadata": {},
   "source": [
    "# 5. Search Engine\n",
    "This module is implemented on a separate EC2 instance acting as the server of a search App. \n",
    "I retrieved data from two DynamoDB tables for TFIDF calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24de604b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
